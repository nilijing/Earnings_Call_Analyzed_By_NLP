# -*- coding: utf-8 -*-
"""StockInfoCollector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18qJ8LtCJW_u26H2_frVtbCNh5fm3D9WS

# StockInfoCollector.py
# Get description of stocks based on ticker details
"""

#!pip install selenium
#!pip install nameparser
#!pip install textblob
#!pip install sumy
#!apt install chromium-chromedriver
#!pip install yfinance

#!cp /usr/lib/chromium-browser/chromedriver /usr/bin

#import sys
#sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

import requests
import time
from bs4 import BeautifulSoup

import os

import pandas as pd
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.actions.interaction import KEY
from selenium.webdriver.common import keys
import json

import yfinance as yf

def grab_page(url,company_ticker,company_name,dfContent):
    #Chrome Session
    #driver=webdriver.Chrome()
    driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
    driver.get(url)
    driver.implicitly_wait(100)
    print("attempting to grab page: " + url)
    # page = requests.get(url)
    # page_html = page.text
    #soup = BeautifulSoup(page_html, 'html.parser')
     
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    content_Sector = soup.find("td", text="Sector:").find_next_sibling("td").text
    content_Industry=soup.find("td", text="Industry:").find_next_sibling("td").text
    driver.quit()
    dfContent = dfContent.append({'StockIndex': company_ticker, 
                                  'StockName': company_name,
                                  'Sector': content_Sector,
                                  'Industry':content_Industry
                                  },ignore_index=True)
    return dfContent
    
    
def process_list_page(Alphabet,dfContent):   
    #driver=webdriver.Chrome()
    driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
    origin_page = "http://eoddata.com/stocklist/NASDAQ/"+str(Alphabet)+".htm"
    print("getting page " + origin_page)
    driver.get(origin_page)
    driver.implicitly_wait(100)
     
    # page = requests.get(origin_page)
    # page_html = page.text
    #print(page_html)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    alist = soup.find_all("table",{'class':'quotes'})
   
    urls_list = alist[0].find_all("a")
    for i in range(0,int(len(urls_list)/3)):
        url_ending=urls_list[3*i].attrs['href']
        url = "http://eoddata.com" + url_ending
        company_ticker=urls_list[3*i].text
        company_name=company_ticker
        if(len(company_ticker)>1):
            company_name=soup.find("td", text=company_ticker).find_next_sibling("td").text
        
        dfContent=grab_page(url,company_ticker,company_name,dfContent)
        time.sleep(.5)
            
    driver.quit()
    return dfContent


def Get_from_Yahoo(Stock_Tickers):   
    dfContent = pd.DataFrame(columns=['StockIndex', 'StockName','Sector', 'Industry'])
    for i in range(len(Stock_Tickers)):
        print(Stock_Tickers['Symbol'][i])
        try:
            ticker = yf.Ticker(Stock_Tickers['Symbol'][i])
            dfContent = dfContent.append({'StockIndex': Stock_Tickers['Symbol'][i], 
                                          'StockName': ticker.info['longName'],
                                          'Sector': ticker.info['sector'],
                                          'Industry':ticker.info['industry']
                                          },ignore_index=True )           
        except:
                print("Error in " +str(Stock_Tickers['Symbol'][i])) 
    
    return dfContent


def Get_from_EOD(Stock_Tickers):   
    #driver=webdriver.Chrome()
    driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
    origin_page = "http://eoddata.com/stocklist/NASDAQ/"+str(Stock_Tickers[0][0])+".htm"
    print("getting page " + origin_page)
    driver.get(origin_page)
    driver.implicitly_wait(100)
     
    # page = requests.get(origin_page)
    # page_html = page.text
    #print(page_html)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    alist = soup.find_all("table",{'class':'quotes'}) 
    urls_list = alist[0].find_all("a")
    for i in range(0,int(len(urls_list)/3)):
        url_ending=urls_list[3*i].attrs['href']
        url = "http://eoddata.com" + url_ending
        company_ticker=urls_list[3*i].text
        dfContent=pd.DataFrame()
        if(company_ticker==Stock_Tickers[0]):
            company_name=company_ticker
            if(len(company_ticker)>1):
                company_name=soup.find("td", text=company_ticker).find_next_sibling("td").text
        
            dfContent=grab_page(url,company_ticker,company_name,dfContent)
            driver.quit()
            return dfContent                
            
    driver.quit()
    return dfContent
    
    
# #Main    
# if __name__ == '__main__':
#     dfContent = pd.DataFrame(columns=['StockIndex', 'StockName','Sector', 'Industry'])
    
    
#     Stock_Tickers=pd.read_excel(open('Stoc_industries.xlsx', 'rb'),
#               sheet_name='Tickers')  

#     for i in range(len(Stock_Tickers)):
#         print(Stock_Tickers['Symbol'][i])
#         try:
#             ticker = yf.Ticker(Stock_Tickers['Symbol'][i])
#             dfContent = dfContent.append({'StockIndex': Stock_Tickers['Symbol'][i], 'StockName': ticker.info['longName'],
#                                   'Sector': ticker.info['sector'],'Industry':ticker.info['industry']},
#                                  ignore_index=True)
            
#         except:
#                 print("Error in " +str(Stock_Tickers['Symbol'][i])) 


# print(sbux.info['sector'])
# print(tlry.info['industry'])
    
    
    
    # Companies=["I","J","K","L","M","N","O","P","Q","R","S","T",
    #            "U","V","W","X","Y"]
    # for alphabet in Companies:
    #     print(alphabet)
    #     try:
    #         dfContent=process_list_page(alphabet,dfContent)
    #     except:
    #         print("Error in " +str(alphabet))

#Stock_Tickers=pd.read_excel(open('/content/drive/MyDrive/Earnings_call_NLP/Stoc_industries.xlsx', 'rb'),sheet_name='Test')

#1.1 Yahoo finance Example API
#Note:confirm BICS Industry classification
#dfContent=Get_from_Yahoo(Stock_Tickers)  
#dfContent

#1.2. EoD Daily scrape 
#Note: Not set to be used for arrays, just being shown as example
#Note: Stocktwits.com get list of key words from last 7 to 14 days. 
#Check Legal and decide whether webscrape
#dfContent=Get_from_EOD(Stock_Tickers['Symbol'])

